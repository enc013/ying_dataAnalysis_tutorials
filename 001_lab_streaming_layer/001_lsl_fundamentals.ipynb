{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0dc19f-63e3-478d-b781-10e9cb451a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pyxdf\n",
    "\n",
    "#Directory where data is located\n",
    "dir_data = 'data'\n",
    "\n",
    "#XDF Files we will use in this tutorial:\n",
    "xdf_oddball = os.path.join(dir_data,'hm_visual_oddball_s01_cond1.xdf')\n",
    "xdf_soundHealing = os.path.join(dir_data,'sound_healing_pilot04_bowls.xdf')\n",
    "xdf_soundHealing_ecg = os.path.join(dir_data,'meditation_music_pilot_tibetinbowls.xdf')\n",
    "xdf_meditation = os.path.join(dir_data,'vrcct_p34_poststudy_condition.xdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9f07e-aa5e-4d75-8385-abb34748ed98",
   "metadata": {},
   "source": [
    "# __Main Points__\n",
    "\n",
    "1. __Lab Streaming Layer__ is an open-source middleware ecosystem to stream, receive, synchronize, and record time series measurements from a wide array of sensor hardware (e.g. EEG headsets, ECG devices, and eye trackers). It has __various programming language interfaces__ (C, C++, Python, Java, C#, MATLAB) and is __cross-platform__ (OS Support: Win / Linux / MacOS / Android / iOS; Architecture Support: x86 / amd64 / arm)\n",
    "\n",
    "2. Data is recorded using __LabRecorder__ and records the data into the __XDF file format (Extensible Data Format)__. You can find more information about this file format [here](https://github.com/sccn/xdf).\n",
    "\n",
    "3. XDF files can be loaded in Python with the package [__pyxdf__](https://github.com/xdf-modules/pyxdf), allowing you analyze it. There are various types of data we need be aware of that include EEG, event markers, eye tracking, and ECG.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cbe366-1d4f-4ccf-9651-b360ea7bd633",
   "metadata": {},
   "source": [
    "# __What is Lab Streaming Layer?__\n",
    "\n",
    "From the [documentation](https://labstreaminglayer.readthedocs.io/info/intro.html), __Lab Streaming Layer__ (__LSL__) is a system for the unified collection of measurement time series in research experiments that handles both networking and time-synchronization (sub-millisecond accuracy). The suite of tools on top of the LSL distribution allow for (near-) real-time access, viewing, and recording of the data. This system was developed by researchers at the Swartz Center for Computational Neuroscience (SCCN). \n",
    "\n",
    "While this definition may sound somewhat nebulous, many (if not all) experiments carried out at the SCCN use LSL. With LSL, researchers can simultaneously record neural, physiological, and behavioral data acquired from a diverse collection of sensor hardware. That is, a single LSL recording, along with metadata, can be comprised of EEG, ECG, eye tracking, and audio data, all of which can have different sampling rates and multiple channels, synchronized to one clock! Moreover, with its various programming language interfaces, we can integrate LSL with hardware API using several lines of code, allowing various kinds of data to be sent via LSL. It can be simple to use. \n",
    "\n",
    "So __how do you record this data?__ One way to record data is through the use of [__LabRecorder__](https://github.com/labstreaminglayer/App-LabRecorder). This LSL application records data into the XDF file format ([Extensible File Format](https://github.com/sccn/xdf)). \n",
    "\n",
    "__Unless you are designing your own experimental pardigm or collecting data, you won't interact with the LSL distribution or Lab Recorder directly__. I will try to provide examples on how you can integrate LSL into any kind of script (using the Python Package pylsl) in case some of you may be interested. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec3e7ab-0c89-4170-b425-fa0ebdbed9f0",
   "metadata": {},
   "source": [
    "# __How do we access this data?__ \n",
    "\n",
    "Before we begin with the tutorial, we need to define a few terms that may be unfamilar with: \n",
    "\n",
    "- __Sample__: A single measurement of all channels from a device. For instance, it can be a single gaze data point from an eye tracker or voltage measurements for all nodes from an EEG headset.\n",
    "- __Metadata__: data that provides information about the stream (see below), but not the actual data (e.g. name, device manufacturer, number of channels, etc.)\n",
    "- __Stream__: The combination of sampled data from a device with the metadata. A stream can have regular sampling rate (e.g. audio can be sampled at 44100Hz) or irregular sampling rate (e.g. key presses or experimental events) with multiple channels (EEG headsets can have 24 channels or more). Each stream is required to have the same data type (integers, floats, doubles, strings). \n",
    "\n",
    "We can access an XDF file with Python using the package __pyxdf__. We have imported it above in the first cell. __This tutorial will show how you can access it and what types of data you may expect.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3530af-24de-4fd0-8380-399e8c589209",
   "metadata": {},
   "source": [
    "## __os - Miscellaneous operating system interfaces__\n",
    "\n",
    "If you haven't worked with loading in files, the Python module __os__ may be new. This module provides ways of using operating system dependent functionality. Your operating system will determine how your paths will be structured: \n",
    "\n",
    "1. For windows, there are drives, backslashes or double backslashes: \"C:\\Users\\JohnSmith\" or \"C:\\\\Users\\\\JohnSmith\"\n",
    "2. For Linus, you don't have the typical drives as in Windows, but use forward slashes: \"/home/JohnSmith\"\n",
    "\n",
    "This is where the Python module os comes in. It allows us to combine strings into a proper path used by your operating system. Rather than guessing if you use a backslash or foward slash (or writing Python code that may be used for other people with different operating systems), you can use os to always have the correct path structure. One function you will see here is __os.path.join__. I encourage you to read the [documentation](https://docs.python.org/3/library/os.path.html). Moreover, you do more than create paths to files. You can check if a file exist (__os.path.isfile__), check if a directory exists (__os.path.isdir__), create new directory (__os.mkdir__), or list all files in a directory (__os.listdir__). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e0468-22a4-464e-a159-3c457d48e68a",
   "metadata": {},
   "source": [
    "# __1 Loading Data with Pyxdf__\n",
    "\n",
    "We will use pyxdf's __load_xdf__ function to see what these mysterious XDF files contain. We can find more information on this function using the help function (__?pyxdf.load_xdf__). From the documentation, this function will return a __list of dictionaries (one for each stream) and a file header as a dictionary__. These streams will have metadata that will help you identify what type of data is represented in the stream. However, __the quality and comprehensiveness of the metadata depends on how careful the device manufacturer or the script author is on documenting the metadata__ (I'll show you an example of how some metadata can be confusing later). Regardless, most streams will have relevant metadata. When the metadata is incomplete or unclear, ask us and we can find out. \n",
    "\n",
    "## __1.1 Accessing Metadata of an XDF file__\n",
    "\n",
    "In the following, we load an XDF file that was recorded for our Sound Healing experiment, where the subject was fitted with a __128 channel Biosemi EEG cap__. The subject was then asked to sit down and listen to __a live performance that was recorded via LSL as well__. There was also an event marker stream that didn't really serve a purpose for this experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d16d47f-7def-4416-a179-98754a15a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will give you more information on the function load_xdf from the pyxdf package (uncomment the next line of code):\n",
    "# ?pyxdf.load_xdf\n",
    "\n",
    "#Loading in streams (we don't really need the file header): \n",
    "streams,_ = pyxdf.load_xdf(xdf_soundHealing)\n",
    "\n",
    "#Each stream is a dictionary. We can access the metadata using the 'info' key. \n",
    "# We can see name, stream type, number of channels, and other information:\n",
    "print('The metadata can have the following information:')\n",
    "print(list(streams[0]['info'].keys()),'\\n')\n",
    "\n",
    "\n",
    "#Printing some relevant metadata to identify \n",
    "print('This XDF File contains the following streams:')\n",
    "for i,stream in enumerate(streams): \n",
    "    print('Stream %i'%(i+1))\n",
    "    print('stream name: %s'%stream['info']['name'][0])\n",
    "    print('stream type: %s'%stream['info']['type'][0])\n",
    "    print('number of channels: %s'%stream['info']['channel_count'][0])\n",
    "    print('channel format: %s'%stream['info']['channel_format'][0])\n",
    "    print('nominal sampling rate: %s'%stream['info']['nominal_srate'][0])\n",
    "    print('Effective (calculated) sampling rate: %f'%stream['info']['effective_srate'],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc7ca5-8017-4bf1-9617-a4db78af913b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## __1.2 Accessing Streams Data__\n",
    "\n",
    "Let's look at the EEG stream (Biosemi). From the metadata, we see that this stream has 137 channels sampled at 2048 Hz. We can access each datum for all channels and its coresponding timestamp (LSL clock). Our Biosemi EEG data is the __second__ dictionary in our __streams__ list. In our stream, we can access it using the '__time_series__' key. It should be an array with N rows (total number of samples) and 137 columns (Number of Channels; 128 EEG channels, 8 external channels, and 1 trigger). We can also access the time stamps of each sample using the dictionary key '__time_stamps__'. Our metadata usually provides us channel information (for multi-channel streams) using the 'desc' key. However, this depends on how well the manufacturer documented its metadata. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9438cd8-688b-4a45-859e-854bd89e6d07",
   "metadata": {},
   "source": [
    "### __1.2.1 EEG__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368559c-d8ef-465e-ade6-184a247526c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access the second stream (EEG) in our xdf file:\n",
    "EEG = streams[1]['time_series'] # EEG data\n",
    "EEG_ts = streams[1]['time_stamps'] #timestamps\n",
    "EEG_meta = streams[1]['info']\n",
    "\n",
    "\n",
    "#Displaying the shape of this array: \n",
    "print('EEG stream has a total of %i samples'%EEG.shape[0])\n",
    "print('EEG stream has %i channels'%EEG.shape[1])\n",
    "print('EEG stream has a total run time of %0.3f seconds \\n'%(EEG_ts[-1]-EEG_ts[0]))\n",
    "\n",
    "#For multi-channel streams, we could access channel labels\n",
    "channel_names = [ chan['label'][0] for chan in EEG_meta['desc'][0]['channels'][0]['channel'] ]\n",
    "print('The channels in the BioSemi EEG cap include: ')\n",
    "print(channel_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9233d2be-8b4e-4caf-8df4-e933ec1ac7a6",
   "metadata": {},
   "source": [
    "### __1.2.2 Audio__\n",
    "\n",
    "We only have one channel for our audio stream since we recorded the live performance with a one microphone. It is the __third stream__ in our streams list. We can access it the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea63628-57c3-463a-ab95-f2c42543c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access the third stream (Audio) in our xdf file: \n",
    "audio = streams[2]['time_series'] # EEG data\n",
    "audio_ts = streams[2]['time_stamps'] #timestamps\n",
    "audio_meta = streams[2]['info']\n",
    "\n",
    "#Displaying the shape of this array: \n",
    "print('Audio stream has a total of %i samples'%audio.shape[0])\n",
    "print('Audio stream has %i channels'%audio.shape[1])\n",
    "print('Audio stream has a total run time of %0.3f seconds \\n'%(audio_ts[-1]-audio_ts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6da269-25f4-43f8-a432-3270ee0692e9",
   "metadata": {},
   "source": [
    "## 1.3 Accessing a Specific Stream\n",
    "\n",
    "You may have noticed that we loaded __ALL STREAMS__ from our XDF recording. Moreover, __if we load another XDF recording from another subject, the order of streams loaded may be different__. For instance, it may have the EEG as our first stream and audio stream as our second stream. Additionally, we don't use the event markers, so we don't want to waste time loading that in. Fortunately, we can load a specific stream(s) using the argument __select_stream__ for the load_xdf function. Again you can use the help function to find more information (__?pyxdf.load_xdf__). To use this, you must know the metadata of the device (it could be as simple as knowing what type and name of stream). If you don't know any metadata of the devices, look at the sections above to display them. Other recordings of the same experiments will be expected to have the same device metadata. Let's demonstrate this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eb2c9b-40a7-4935-9f9a-d5a851bd28c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in EEG stream and accessing data:\n",
    "EEG_stream, _ = pyxdf.load_xdf(\n",
    "    xdf_soundHealing,\n",
    "    select_streams=[{'type':'EEG'}]\n",
    ")\n",
    "\n",
    "EEG = EEG_stream[0]['time_series']\n",
    "EEG_ts = EEG_stream[0]['time_stamps']\n",
    "\n",
    "#Loading in Audio stream and accessing data:\n",
    "audio_stream, _ = pyxdf.load_xdf(\n",
    "    xdf_soundHealing,\n",
    "    select_streams=[{'type':'Audio'}]\n",
    ")\n",
    "audio = audio_stream[0]['time_series']\n",
    "audio_ts = audio_stream[0]['time_stamps']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d0ced-1ec2-44d2-b44b-be901061806d",
   "metadata": {},
   "source": [
    "# __2 What type of data will you encounter?__\n",
    "\n",
    "Depending on the experiment, you may be asked to analyze different kinds of data. While not a comprehensive list, there are four that I encounter: 1) EEG, 2) Event Markers, 3) Eye Tracking, 4) ECG, and 5) Audio. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544e3da9-a1ea-4544-8a46-8b68fdc9ab99",
   "metadata": {},
   "source": [
    "## 2.1 EEG\n",
    "\n",
    "EEG is the most prevalent data you will encounter (duh! we are working for the Swartz Center for Computational Neuroscience). However, we don't really do EEG data analysis in Python and is mainly done in MATLAB using EEGLab. If you want to explore EEG data analysis in Python, check out [MNE-Python](https://mne.tools/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de26c5-59fa-4c28-a0ba-091100bb3b95",
   "metadata": {},
   "source": [
    "## 2.2 Event Markers\n",
    "\n",
    "Event Markers are used to annotate our data. They can be strings or integers. They mark when an event happens in the experiment. For instance, we can send event markers when the experiment starts and ends. We can even send event markers when the subject presses keys on the keyboard. You may notice that these types of streams have an irregular sampling rate (sampling rate of 0 Hz; see above in metadata section). We can use these events to analyze specific sections of an experiment. \n",
    "\n",
    "Let's look at a recording. For an oddball experiment, there were two different types of cubes (standard and deviant) shown to a subject in virtual reality at four different positions (as indicated in each event). Events are sent as a strings. These events tell you what cube is displayed (standard or deviant), the trial number (number of cubes displayed), and the relative position (in x,y,z coordinates). This experiment is used to characterize fixations, saccades, and smooth pursuits in immersive 3D VR with free head movement (ongoing as of Oct. 2022). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a78cd-a35e-4072-bc20-66998b56839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in Event Markers Stream for oddball\n",
    "event_stream,_ = pyxdf.load_xdf(\n",
    "    xdf_oddball,\n",
    "    select_streams=[{'name':'EventMarker'}]\n",
    ")\n",
    "\n",
    "#note that string markers are stored as lists, so we use list iteration\n",
    "events = [event[0] for event in event_stream[0]['time_series']]\n",
    "events_ts = event_stream[0]['time_stamps']\n",
    "\n",
    "#Displaying an event\n",
    "# We have trial number, type of cube, and coordinate\n",
    "# We also have event markers when this trial ends\n",
    "print(events[1])\n",
    "print(events[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77da60ff-1285-4caa-bb00-3e652f098801",
   "metadata": {},
   "source": [
    "## __2.3 Eye Tracking__\n",
    "\n",
    "Eye tracking data can have many channels that correspond to 2D gaze points, 3D gaze points, angular velocity, etc. Gaze data can be used to identify fixations (which are adjacent collections of gaze points that are below certain thresholds). Fixations can then be seen as events, allowing us to analyze sections of EEG and plotting fixation related potentials (FRPs).\n",
    "\n",
    "The oddball experiment that was described in the previous section does have an eye tracking stream. However, it has poorly annotated metadata. We don't have any information on the channels and rely on our notes to see what each channel contains. Also notice that the sampling rate is 90Hz instead of the expected 50Hz (which is indicated by the metadata). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6f5f4b-7c83-482c-ba52-c23c5951eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in Event Markers Stream for oddball\n",
    "gaze_stream,_ = pyxdf.load_xdf(\n",
    "    xdf_oddball,\n",
    "    select_streams=[{'name':'ProEyeGaze'}]\n",
    ")\n",
    "\n",
    "#2d Gaze coordinates of the left eye are contained in channels 1 and 2:\n",
    "gaze2d_left = gaze_stream[0]['time_series'][:,:2]\n",
    "print('Left Eye 2D Coordinates')\n",
    "print(gaze2d_left[0,:],'\\n') #first sample\n",
    "\n",
    "#3d Gaze coordinates of the left eye are contaiend in channels 4,5,6\n",
    "print('Left Eye 3D Coordinates')\n",
    "gaze3d_left = gaze_stream[0]['time_series'][:,3:6]\n",
    "print(gaze3d_left[0,:]) #first sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761ddd8b-394e-4a29-90cd-d19b1fcfa4da",
   "metadata": {},
   "source": [
    "## __2.4 ECG__\n",
    "\n",
    "ECG can be a little tricky to extract sometimes because some EEG headsets have external channels that can be used to measure ECG. There is no dedicated ECG stream in this case. Other times, we do have a dedicated ECG stream with one (or two) channels. \n",
    "\n",
    "I will display these two types of scenarios. The first case is from our Sound Healing pilot study (briefly described in __Section 1.1__) where we wanted to measure ECG of a subject listening to a concert recording. It has a dedicated ECG stream (from the HeartyPatch device). We have two channels with the ECG data and the device timestamps the sample was collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f40b54c-6aa4-4861-888b-9262b3adeb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recording with a dedicated ECG stream \n",
    "ECG_stream,_ = pyxdf.load_xdf(\n",
    "    xdf_soundHealing_ecg,\n",
    "    select_streams=[{'type':'ECG'}],\n",
    ")\n",
    "\n",
    "#Raw ECG data: first channel\n",
    "ECG = ECG_stream[0]['time_series'][:,0]\n",
    "ECG_ts = ECG_stream[0]['time_stamps']\n",
    "\n",
    "#Plotting first 10 seconds of recording: \n",
    "bool_array = (ECG_ts>=ECG_ts[0]) & (ECG_ts<=ECG_ts[0]+10)\n",
    "\n",
    "fig1,ax1 = plt.subplots(figsize=(7,5))\n",
    "ax1.plot(ECG_ts[bool_array],ECG[bool_array])\n",
    "ax1.set_title('Sound Healing')\n",
    "ax1.set_xlabel('LSL Clock Time (seconds)')\n",
    "ax1.set_ylabel('ECG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea676fce-6b38-4928-9ed1-bc0e75476120",
   "metadata": {},
   "source": [
    "Next, we will look at a recording from our VRCCT experiment where subjects were in a VR space and guided through a meditation session. ECG (from the left and right arm) is in the EEG stream on its external channels (66 and 67). We will load the EEG stream and then extract the ECG data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb39165-ebd6-4e44-8dae-b087623a57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in EEG stream:\n",
    "EEG_stream, _ = pyxdf.load_xdf(\n",
    "    xdf_meditation,\n",
    "    select_streams=[{'type':'EEG'}]\n",
    ")\n",
    "\n",
    "#Extracting EEG:\n",
    "EEG = EEG_stream[0]['time_series']\n",
    "\n",
    "#Extracting ECG (channel 66) from the EEG data:\n",
    "ECG = EEG[:,65]\n",
    "\n",
    "#Timestamps correspond to both EEG and ECG:\n",
    "timestamps = EEG_stream[0]['time_stamps']\n",
    "\n",
    "#Plotting first 10 seconds of recording: \n",
    "bool_array = (timestamps>=timestamps[0]) & (timestamps<=timestamps[0]+10)\n",
    "\n",
    "fig1,ax2 = plt.subplots(figsize=(7,5))\n",
    "ax2.plot(timestamps[bool_array],ECG[bool_array]) #note the baseline wander! \n",
    "ax2.set_title('VRCCT Experiment')\n",
    "ax2.set_xlabel('LSL clock Time (seconds)')\n",
    "ax2.set_ylabel('ECG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16b6933-6675-494a-9553-e69c4e17e1e8",
   "metadata": {},
   "source": [
    "## __2.5 Audio__\n",
    "\n",
    "One of our projects involve subjects listening to audio. We may need to do some analysis on these audio stream (creating spectrograms of the music or pinpointing certain music features that we are interested in). We can use Python packages like [__librosa__](https://librosa.org/doc/latest/index.html) to analyze such data. These types of streams are characterized by a large sampling rate (in the case of our Sound Healing experiment, we sampled at 11025 Hz for one channel). I mainly analyze these types of data in conjunction with event markers. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
